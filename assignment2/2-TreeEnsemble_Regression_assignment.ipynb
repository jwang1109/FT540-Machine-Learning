{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3ab9b3c-432a-4e97-8555-f4ed7c4c61f2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "# Second Assignment - FINTECH 540 - Machine Learning for FinTech - Cryptocurrency Options Data analysis\n",
    "\n",
    "In this assignment, you'll work with cryptocurrency options transaction data between 2022-08-04 and 2022-11-18 from Binance. The primary objective is to achieve a satisfactory performance in predicting option prices. This is a regression task, and you must ensure that your model can predict well on the test set (out-of-sample).\n",
    "\n",
    "## Dataset Overview\n",
    "\n",
    "You have been provided with a dataset containing the following columns:\n",
    "\n",
    "- **Symbol**: Unique identifier for each option contract. For instance, the option contract *ETH-221125-900-C* refers to a Call option (C) for Ethereum (ETH), with a strike price (also called execution price) of 900, expiring on November 25th, 2022 (221125. If a holder has this option, they can buy Ethereum at a price of 900 units of currency on or before this expiration date, regardless of Ethereum's actual market price.\n",
    "- **Time**: Timestamp indicating when the trade occurred.\n",
    "- **Price**: Price at which the option was traded. **This is the target you want to predict for this task**.\n",
    "- **Quantity (Qty)**: Number of option contracts traded.\n",
    "- **Strike Price**: The predetermined price at which the holder can buy or sell the underlying asset.\n",
    "- **Underlying Price**: The current market price of the underlying asset (e.g., BTC or ETH for this dataset).\n",
    "- **Time to Maturity (Tt)**: Duration (expressed as a fraction of a year) until the option's expiration date. For instance, if `Tt=0.145152`, there are 0.145152*365 = 52.98 days until the expiration. The calculation of the Time to Maturity has already been provided to you, and you do not need to bring it back to several days for the sake of this exercise. \n",
    "- **RV_lag0, RV_lag7, RV_lag15, RV_lag30**: Realized underlying asset volatility at various lag values. The lag 0 reflects the underlying asset's volatility on the day the transaction occurred.\n",
    "- **Underlying Asset**: The asset upon which the option contract is based (e.g., BTC or ETH).\n",
    "- **Option Type**: Specifies whether it's a 'Call' (C) or 'Put' (P) option.\n",
    "- **Side**: Represents whether the trade was a buy (`1`) or a sell (`-1`) since these are transaction data that reflect both sides of the order book.\n",
    "\n",
    "## Task and General Hints\n",
    "\n",
    "In this assignment, you are tasked with building a predictive regression model on cryptocurrency options data. Your primary goal is to ensure accurate out-of-sample predictions and evaluate them with the metrics below.\n",
    "\n",
    "To guide you through this process, consider breaking down your tasks into the following three phases:\n",
    "\n",
    "**Preprocessing**\n",
    "The dataset is already free of inconsistencies, missing values, or outliers. \n",
    "- **Feature Engineering**: You might want to create additional variables, perform transformations, or encode categorical variables if necessary. Ensure that all the variables you want to use for modeling are correctly preprocessed. You don't need to use all the variables necessarily. You will eventually refine your choices while modeling.\n",
    "- **Data Splitting**: Partition your data into training and test sets. Ensure you set the seed to `42` for reproducibility and use `0.33` as the test size.\n",
    "\n",
    "**Model Selection**\n",
    "- This notebook focuses on using ensembles of trees for regression. You can experiment with all the ensembles we have seen in class. However, feel free to compare the performance against a linear regression model. \n",
    "\n",
    "**Model Tuning and Evaluation**\n",
    "- Once you've selected a model, you'll want to fine-tune its parameters to achieve the best out-of-sample performance.\n",
    "- You may adjust parameters manually, but consider using Grid Search or Randomized Search for a more systematic and potentially practical approach. \n",
    "- When employing Grid Search (or Randomized Search), you can use cross-validation schemes provided in **Notebook 8**.\n",
    "- Evaluate your final model using the $R^2$ and the Root Mean Squared Error (RMSE) metrics from `scikit-learn.` For the RMSE, you can either calculate the MSE and take the square root of it or use the `mean_squared_error` function from `scikit-learn` and pass the parameter `squared=False.` Remember, this is the primary criterion on which you will be graded. You can carry out the calculations on your own while developing your solution. However, the final cell of this notebook is also going to take care of it, so follow the naming convention stated at the bottom of the notebook.\n",
    "\n",
    "**Note**: Parameter choices and tuning should be made thoughtfully while up to you. Carefully study the documentation of the tree ensemble you are testing to see the possible parameters you can fine-tune. In notebook 8, you have the structure of a simple grid search over a small grid of parameters. You can borrow that structure and modify it accordingly.\n",
    "\n",
    "**IMPORTANT REMARK**: \n",
    "\n",
    "In scikit-learn, if you use cross-validation functions like `cross_val_score` or `GridSearchCV` or `RandomizedGridSearchCV` with a specified cross-validation scheme, you only need to pass the training dataset. The cross-validation function will automatically split the training data into training and validation subsets multiple times according to the selected cross-validation strategy. Therefore, creating a validation set to tune the hyperparameters is unnecessary.\n",
    "\n",
    "Instead, you have to use the test set obtained from the split solely as data the model has never seen before. The results on that part of the dataset are those that are going to provide your grade.\n",
    "\n",
    "## Grading Rubric\n",
    "\n",
    "Your grade will be determined by combining the $R^2$ value and the **normalized Root Mean Squared Error (RMSE)** your model achieves on the test set. Specifically, your grade will be calculated as:\n",
    "\n",
    "$$ \\text{Grade} = (0.7 \\times R^2 + 0.3 \\times \\text{Normalized RMSE}) \\times 100 $$\n",
    "\n",
    "which will be a number between 0 and 100. Grades may be curved before being released.\n",
    "\n",
    "The normalization for RMSE is defined as:\n",
    "\n",
    "$$ \\text{Normalized RMSE} = 1 - \\left( \\frac{\\text{RMSE}}{\\text{MAX_POSSIBLE_RMSE}} \\right) $$\n",
    "\n",
    "Where `MAX_POSSIBLE_RMSE` represents a domain-specific value that signifies the worst possible RMSE for your dataset, which could be set as the standard deviation of the target variable. This normalization ensures that the RMSE value is scaled between 0 (worst) and 1 (best).\n",
    "\n",
    "**Rationale**\n",
    "\n",
    "The rationale for using both $R^2$ and **RMSE** in your grade is to ensure a holistic assessment of your model's performance:\n",
    "\n",
    "- **$R^2$** captures the proportion of the variance in the dependent variable that is predictable from the independent variables. A higher $R^2$ indicates a model that explains more of the variation, providing insight into the model's goodness of fit.\n",
    "  \n",
    "- **RMSE** measures the average magnitude of the errors between predicted and observed values. It offers a more direct interpretation of how much, on average, predictions deviate from the actual values, allowing for a clear understanding of the model's accuracy. The RMSE is normalized to allow being combined with the $R^2$ and results in a number between 0 and 100.\n",
    "\n",
    "By weighting $R^2$ and **RMSE** with weights of 0.7 and 0.3, respectively, we emphasize the model's ability to explain variance while holding it accountable for its accuracy in terms of error magnitude.\n",
    "\n",
    "The quality of the prediction assessed by those metrics will result from all the choices you made when it comes to preprocessing features, including them into a model, selecting and evaluating a proper regression model, and eventually doing hyperparameter optimization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75d1e90f-9e63-4138-81f6-4dd08fd51d1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef90e0c3-26e1-4240-b0fb-71cf99a86e51",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v8/xqbjq3fj21n487f3cs1cthq00000gn/T/ipykernel_12358/385938813.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['trans_underlying_asset'] = df['underlying_asset'].apply(lambda x: 1 if x == \"ETH\" else 0)\n",
      "/var/folders/v8/xqbjq3fj21n487f3cs1cthq00000gn/T/ipykernel_12358/385938813.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['trans_option_type'] = df['option_type'].apply(lambda x: 1 if x == \"C\" else 0)\n"
     ]
    }
   ],
   "source": [
    "raw_data = pd.read_csv('raw_options_binance.csv')\n",
    "# 1. data preprocessing\n",
    "# 1.1 selecet proper columns\n",
    "# Time and Symbol looks useless here.\n",
    "# The first column is just index, should drop it\n",
    "\n",
    "selected_columns = [ 'price', 'qty', 'strike_price',\n",
    "       'underlying_price', 'Tt', 'RV_lag0', 'RV_lag7', 'RV_lag15', 'RV_lag30',\n",
    "       'underlying_asset', 'option_type', 'side']\n",
    "df = raw_data[selected_columns]\n",
    "# 1.2 transform time underlying_asset and option_type into numerical form that a model can interpret.\n",
    "df['trans_underlying_asset'] = df['underlying_asset'].apply(lambda x: 1 if x == \"ETH\" else 0)\n",
    "df['trans_option_type'] = df['option_type'].apply(lambda x: 1 if x == \"C\" else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ac5be57-6b54-4d80-9c42-ec996be8adbf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['option_type'].unique()\n",
    "X = df[['qty', 'strike_price',\n",
    "       'underlying_price', 'Tt', 'RV_lag0', 'RV_lag7', 'RV_lag15', 'RV_lag30',\n",
    "       'trans_underlying_asset', 'trans_option_type', 'side']]\n",
    "y = df['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "179f9947-1e19-49c3-ac0c-22542f0d3d7e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27731, 5)\n",
      "Fitting 5 folds for each of 56 candidates, totalling 280 fits\n",
      "number of components from PCA: 5\n",
      "Best parameters: {'learning_rate': 0.4, 'max_depth': 8, 'n_estimators': 50}\n",
      "Neg MSE (Training set): -12050.5804\n",
      "(27731, 6)\n",
      "Fitting 5 folds for each of 56 candidates, totalling 280 fits\n",
      "number of components from PCA: 6\n",
      "Best parameters: {'learning_rate': 0.4, 'max_depth': 3, 'n_estimators': 500}\n",
      "Neg MSE (Training set): -10110.2720\n",
      "(27731, 7)\n",
      "Fitting 5 folds for each of 56 candidates, totalling 280 fits\n",
      "number of components from PCA: 7\n",
      "Best parameters: {'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 500}\n",
      "Neg MSE (Training set): -8045.2063\n",
      "(27731, 8)\n",
      "Fitting 5 folds for each of 56 candidates, totalling 280 fits\n",
      "number of components from PCA: 8\n",
      "Best parameters: {'learning_rate': 0.30000000000000004, 'max_depth': 7, 'n_estimators': 500}\n",
      "Neg MSE (Training set): -7250.5993\n",
      "(27731, 9)\n",
      "Fitting 5 folds for each of 56 candidates, totalling 280 fits\n",
      "number of components from PCA: 9\n",
      "Best parameters: {'learning_rate': 0.30000000000000004, 'max_depth': 7, 'n_estimators': 500}\n",
      "Neg MSE (Training set): -7036.1304\n",
      "(27731, 10)\n",
      "Fitting 5 folds for each of 56 candidates, totalling 280 fits\n",
      "number of components from PCA: 10\n",
      "Best parameters: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 500}\n",
      "Neg MSE (Training set): -7201.6816\n",
      "(27731, 11)\n",
      "Fitting 5 folds for each of 56 candidates, totalling 280 fits\n",
      "number of components from PCA: 11\n",
      "Best parameters: {'learning_rate': 0.30000000000000004, 'max_depth': 3, 'n_estimators': 500}\n",
      "Neg MSE (Training set): -7070.9674\n"
     ]
    }
   ],
   "source": [
    "# 1.3 fitting model\n",
    "max_components = min(X.shape)\n",
    "model_results = []\n",
    "for n in range(5, max_components + 1):\n",
    "    # perform PCA\n",
    "    pca = PCA(n_components = n)\n",
    "    pca.fit(X)\n",
    "    trans_X = pca.transform(X)\n",
    "    \n",
    "    # split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(trans_X, y, test_size=0.33, random_state=42)\n",
    "    print(X_train.shape)\n",
    "\n",
    "    # build model\n",
    "    model = XGBRegressor(random_state = 42)\n",
    "    \n",
    "    # fine-tuned\n",
    "    param_grid = {\n",
    "        \"learning_rate\": np.arange(0.1,0.5,0.1),\n",
    "        \"max_depth\": range(3, 10),\n",
    "        \"n_estimators\":[50,500]\n",
    "    }\n",
    "    model_gs = GridSearchCV(\n",
    "        model, param_grid, scoring=\"neg_mean_squared_error\",  n_jobs=-1, verbose=1\n",
    "    )\n",
    "    \n",
    "    # Record the result\n",
    "    model_gs.fit(X_train,y_train)\n",
    "    result = {}\n",
    "    result['PCA_n'] = n\n",
    "    result['best_params'] = model_gs.best_params_\n",
    "    result['best_score'] = model_gs.best_score_\n",
    "    model_results.append(result)\n",
    "    print(\"number of components from PCA:\",n)\n",
    "    print(f\"Best parameters: {model_gs.best_params_}\")\n",
    "    print(f\"Neg MSE (Training set): {model_gs.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "413edfea-a691-4a06-9b78-655657c81995",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'PCA_n': 9, 'best_params': {'learning_rate': 0.30000000000000004, 'max_depth': 7, 'n_estimators': 500}, 'best_score': -7036.130389498916}\n"
     ]
    }
   ],
   "source": [
    "max_dict = max(model_results, key=lambda x: x['best_score'])\n",
    "print(max_dict)  # This will output {'name': 'Jane', 'score': 95}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aea5af65-6a2c-406e-8023-4d1c67bb6900",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#1.4 reproduce the model to get a grade\n",
    "\n",
    "\n",
    "pca = PCA(max_dict['PCA_n'])\n",
    "pca.fit(X)\n",
    "trans_X = pca.transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(trans_X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "best_model = XGBRegressor(\n",
    "    learning_rate = max_dict['best_params']['learning_rate']\n",
    "    ,max_depth = max_dict['best_params']['max_depth']\n",
    "    ,n_estimators = max_dict['best_params']['n_estimators']\n",
    "    ,random_state=42\n",
    ")\n",
    "best_model.fit(X_train, y_train)\n",
    "y_test_pred = best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d33b67-700b-44ad-949c-d70ed2667150",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "**Instructions to let the next code cell run:**\n",
    "\n",
    "Before running the cell below, ensure the following:\n",
    "1. The target variable of your problem has to be named exactly `y_test`, while the out-of-sample prediction variable has to be named `y_test_pred`. Also the calculation of `MAX_POSSIBLE_RMSE` relies on this naming convention to determine the standard deviation of the test target values. \n",
    "\n",
    "By adhering to these naming conventions, the grading cell can compute the final score without any issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35dd6ac5-e25f-4866-94b6-3773918dda59",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The grade for this assignment is  97\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "MAX_POSSIBLE_RMSE = y_test.std()\n",
    "normalized_rmse = 1 - (mean_squared_error(y_test,y_test_pred,squared=False) / MAX_POSSIBLE_RMSE)\n",
    "R2 = r2_score(y_test,y_test_pred)\n",
    "\n",
    "Grade = 0.7 * R2 + 0.3 * normalized_rmse\n",
    "print('The grade for this assignment is ',math.ceil(Grade*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "classenv",
   "language": "python",
   "name": "classenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
